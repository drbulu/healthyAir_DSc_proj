---
title: "Pollution Data Preparation - Strategy and Implementation"
output:
  html_document:
    toc: true
    toc_depth: 4
---

## Introduction

### Chosen dataset

The [selected dataset](https://www.epa.gov/sites/production/files/2016-12/state_tier1_90-16.xls) contained an annual summary of of a number of pollutants by source, pollutant time and state. This dataset was obtained from the [Air Pollutant Emissions Trends page](https://www.epa.gov/air-emissions-inventories/air-pollutant-emissions-trends-data), which also hosted a similar dataset which contained aggregated national figures by source and pollutant. The national dataset covered a longer timeframe but was much less useful because we need state level data in order to integrate pollution data with the asthma and traffic datasets.

Therefore, compared to the [Asthma](https://github.com/drbulu/healthyAir_DSc_proj/blob/master/01_Data_Prep/asthma_data_source_prep_02.Rmd) and [Traffic](https://github.com/drbulu/healthyAir_DSc_proj/blob/master/01_Data_Prep/traffic_data_source_prep_01.Rmd) this challenge was arguably simpler. However, it is important to keep in mind that this dataset has undergone what appears to be a substantial amount of preparation. This was determined by looking at the README excel sheet in the selected state-level dataset which described how the dataset was prepared. Some things to note are:

* This data set was fused from other data sources, as described, and interpolation was used to fill in data availability gaps from years where no source data was available.

* Some definitions change over time and some level of recalculation was required to standardise some values. For example, we may want to merge the "wild fires"", "prescribed fires"" and "misc" variables into a new FIRE_AND_MISC variable, creating a new identifier from the combination of the respective codes.

This is probably one of the reasons why this datset is described as ["trend level data"](https://www.epa.gov/air-emissions-inventories/pollutant-emissions-summary-files-earlier-neis).

However, as mentioned before, the granularity (detail level) is comparable to the other datasets that we aim to look at. In addition, a large number of interesting variables are also included. This, therefore, provides a good introduction to the potential value of this dataset while giving us a needed break from the data crunching grind :scream:.

### Further data sources

#### EPA Pre-Generated Data Files

The section of [Pre-Generated Data Files](https://aqsdr1.epa.gov/aqsweb/aqstmp/airdata/download_files.html) contains some potentially interesting additional data. In particular, the meterological and particulate data in the [daily section](https://aqsdr1.epa.gov/aqsweb/aqstmp/airdata/download_files.html#Daily), and the daily Particulates datasets may be worthwhile investigating as [weather](http://www.abc.net.au/news/2016-11-22/two-die-in-thunderstorm-asthma-emergency-in-melbourne/8044558) and [particulates](http://www.tandfonline.com/doi/abs/10.1080/00039896.1993.9938391) may be linked to respiratory health.

There is a description of the meaning behind the [wind-related variables](https://www.epa.gov/aqs/aqs-wind-speed-and-direction-parameter-change) for reference. If monthly granularity was critical for analysis or if meterological information was required, this archive is a good place to visit. However, a lot of aggregation would be required to get a decent summary by state as outlined above. This maybe worth a look if initial analysis and modelling reveals the need for more specific, or additional information.

#### Comprehensive meterological data

Precipitation data was oddly absent from the Pre-Generated Data Files. However, there is some [weather station](https://www.ncdc.noaa.gov/data-access/land-based-station-data) data that might be useful. The datasets appear to be both rich and raw, including many other parameters, and data is available via FTP with [metadata descriptions](ftp://ftp.ncdc.noaa.gov/pub/data/asos-fivemin/td6401-1.txt). The weather station data potentially useful, but its processing would need to be motivated by a rather compelling reason, given the apparent complexity of the task at hand. 

There also exists other information on [storm events](https://www.ncdc.noaa.gov/stormevents/ftp.jsp) and [hourly precipitation](ftp://ftp.ncdc.noaa.gov/pub/data/hourly_precip-3240/readme.txt).

Lastly, another potentially interesting, comprehensive data source is the [Integrated Surface Database](https://www.ncdc.noaa.gov/isd/data-access) (ISD) [ftp data source] (ftp://ftp.ncdc.noaa.gov/pub/data/noaa/) with a [readme](ftp://ftp.ncdc.noaa.gov/pub/data/noaa/readme.txt), [detailed metadata](ftp://ftp.ncdc.noaa.gov/pub/data/noaa/ish-format-document.pdf) that seems to match individual datasets, [country list](ftp://ftp.ncdc.noaa.gov/pub/data/noaa/country-list.txt), [data description](ftp://ftp.ncdc.noaa.gov/pub/data/noaa/ish-abbreviated.txt)
and a file that appears to contain a [list](ftp://ftp.ncdc.noaa.gov/pub/data/noaa/isd-history.txt) of weather stations. There is is even a [lite-version](ftp://ftp.ncdc.noaa.gov/pub/data/noaa/isd-lite/) that appears to be more user friendly, and thus appealing, dataset according to the [formatting](ftp://ftp.ncdc.noaa.gov/pub/data/noaa/isd-lite/isd-lite-format.txt) and [technical](ftp://ftp.ncdc.noaa.gov/pub/data/noaa/isd-lite/isd-lite-technical-document.txt) documentation. Of all the data source alternatives, this seems to be a good one :triumph: :joy:! 

Basically, there are many more data sources that that could provide more information dimensions to the data that we have obtained thus far. However, these would need further aggregation that would seem excessive for our current purposes.

That said, there is a tutorial for how to process the ISD data [here](http://blue.for.msu.edu/lab-notes/NOAA_0.1-1/NOAA-ws-data.pdf), and one at [r-bloggers](https://www.r-bloggers.com/parse-noaa-integrated-surface-data-files/). Encouragingly, a couple of packages exist that appear to be devoted to analysis of this data include [rnoaa](https://cran.rstudio.com/web/packages/rnoaa/), [isdparser](https://cran.rstudio.com/web/packages/isdparser/).

#### Potential additional data candidates

Overall, the following data sources seem to be most useful to keep in mind when looking to expand the data analysis in future:

* Pre-Generated Data Files: [Daily](https://aqsdr1.epa.gov/aqsweb/aqstmp/airdata/download_files.html#Daily) pollution and partial meterological information (rainfall missing)

* Integrated Surface Database (particularly **ISD-lite**): comprehensive basic [meterological](ftp://ftp.ncdc.noaa.gov/pub/data/noaa/isd-lite/) information.

* Storm events (if not covered in ISD database): metadata [here](ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/ugc_areas.csv).

Upon reflection, the weather data probably deserves its own section (if not project).

## Strategy

Thought about leaving the data as is. however, tidying the data would be useful in preparation for exploration... helps us to focus on the task at hand.

Split data into two files: 

* metadata
* emissions_data

### Metadata

3 Pairs of descriptors

Source location metadata <!-- use proper HTML tables and set width = 50% -->

| State | FIPS_CODE |
|-------|------|
| AL | 01 |

Pollutant type metadata

| Pollutant | Unit |
|-------|------|
| NOX | 1000 tons |

Pollution sources metadata

| Source | Tier1_CODE |
|-------|------|
| AL | 01 |

Using the above examples:

| Type  | Entity | Desc |
|-------|--------|-------|
| State | AL     | FIPS_CODE=01 |
| Pollutant | NOX     | unit=1000 tons |
| Source | 01 | FUEL COMB. ELEC. UTIL. |

This summary metadata effectively condenses the accessory information to a more understand and simplified form. 

This also has a number of important benefits:

1) Reduces the amount of redundancy in the main data table, thus making the overall file size smaller, even though we have 2 files instead of one.

2) Allows the remaining information to be transposed into a tidy data format ammenable for use in steps such as visulisation using **ggplot2**.

column names become concatenated metadata codes e.g. STATE.SOURCE.Pollutant, which would be represented for example as AL.01.NOX using the sample tables above. 

* care take to make sure these resulting column names are legal
* can then use the metadata table to link the relevant abbreviations in the main data to additional related information.


Initially, processing requirements for this data set appeared simple on face value. This was due to the fairly processed nature of the dataset. However, the need to prepare each dataset to minimise the need for data manipulation in downstream exploration and analysis complicated things.

The structure of the original data set is not ammenable to this:


The first solution is the function f(x) to process and transpose the data so that the Year information is extracted from the table names, and moved to its own column (variable). However, We have converted a dataset with approx 5321 rows and 27 cols, to one that has 22 rows and 5322 cols. This is slightly smaller and more useful, becasue years are in their own column

```{r, eval=F}
epaExpandTimeSeriesByID = function(epaDataSubset){
    # expand column title into descriptor variables
}
epaReformatTimeSeriesData = function(epaData, byPollutant = T){
    # expand column title into descriptor variables
    # for a group of different columns, using the
    # epaExpandTimeSeriesByID() function, then merge
    # the data into a single output
}
```

## Implementaion

pollution_data_source_prep_01.R

helpers - helpers-epa_data_prep_01.R



#### Comments about extra functionality

extra functionality: helpers-epa_data_explore_01.R

born from an attempt to further process the transformed data table


These functions Expand the variables in the transposed data summary created
during the data preparation phase of the EPA State summary dataset.
These functiosn are NOT intended to process the ENTIRE dataset, but to prepare
subsets of interest for exploratory analysis and other analyses. Basically,
the functions, particularly:
  epa_help_explore_01epaReformatTimeSeriesData()
were slow when applied to the roughly 5200 variables in the transposed data set.
Further, the datasets tended to be sparse due to missing values, presumably
because (among other reasons) some pollutants were not emmitted by certain sources.