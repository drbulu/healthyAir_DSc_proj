---
output:
  html_document
---
# Pollution Data Preparation I - Data Processing Strategy

## Introduction

By state (national is not helpful)

https://www.epa.gov/sites/production/files/2016-12/state_tier1_90-16.xls


bigger challenge: There are many more data sources that would need aggregation, however, this seems excessive for our current purposes. If monthly granularity was critical for analysis, this archive is the place to visit. A lot of aggregation would be required to get a decent summary by state as outlined above. This maybe worth a look if initial analysis and modelling reveals the need for more specific information


https://aqsdr1.epa.gov/aqsweb/aqstmp/airdata/download_files.html

https://www.epa.gov/aqs/aqs-wind-speed-and-direction-parameter-change

more detailed wind data source. Also includes many other parameters lots of data available via FTP, potentially useful, but 
 
https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/automated-surface-observing-system-asos
https://www.ncdc.noaa.gov/data-access/land-based-station-data

ftp://ftp.ncdc.noaa.gov/pub/data/asos-fivemin/td6401-1.txt


However, there is some potentially interesting meterological data in the daily section for future use, and the daily Particulates datasets may be worthwhile investigating as they may be linked to respiratory 

http://www.abc.net.au/news/2016-11-22/two-die-in-thunderstorm-asthma-emergency-in-melbourne/8044558 

* particulates vs respiratory health

http://europepmc.org/abstract/med/7492903

http://www.tandfonline.com/doi/abs/10.1080/00039896.1993.9938391

* economic status

http://www.tandfonline.com/doi/abs/10.1080/00039896.1967.10664708



## Strategy

Split data into two files: 

* metadata
* emissions_data

### Metadata

Note: readme sheet of the source data gives an interesting insight into data creation and possible issues. Major note, this data set fused from other data sources, and interpolation used to fill in gaps. 

Some definitions change over time, consequently, may need to fuse wild fires, prescribed fires and misc into FIRE_AND_MISC and combine the respective codes into a new identifier.

3 Pairs of descriptors

Source location metadata <!-- use proper HTML tables and set width = 50% -->

| State | FIPS_CODE |
|-------|------|
| AL | 01 |

Pollutant type metadata

| Pollutant | Unit |
|-------|------|
| NOX | 1000 tons |

Pollution sources metadata

| Source | Tier1_CODE |
|-------|------|
| AL | 01 |

Using the above examples:

| Type  | Entity | Desc |
|-------|--------|-------|
| State | AL     | FIPS_CODE=01 |
| Pollutant | NOX     | unit=1000 tons |
| Source | 01 | FUEL COMB. ELEC. UTIL. |

This summary metadata effectively condenses the accessory information to a more understand and simplified form. 

This also has a number of important benefits:

1) Reduces the amount of redundancy in the main data table, thus making the overall file size smaller, even though we have 2 files instead of one.

2) Allows the remaining information to be transposed into a tidy data format ammenable for use in steps such as visulisation using **ggplot2**.

column names become concatenated metadata codes e.g. STATE.SOURCE.Pollutant, which would be represented for example as AL.01.NOX using the sample tables above. 

* care take to make sure these resulting column names are legal
* can then use the metadata table to link the relevant abbreviations in the main data to additional related information.


Initially, processing requirements for this data set appeared simple on face value. This was due to the fairly processed nature of the dataset. However, the need to prepare each dataset to minimise the need for data manipulation in downstream exploration and analysis complicated things.

The structure of the original data set is not ammenable to this:


The first solution is the function f(x) to process and transpose the data so that the Year information is extracted from the table names, and moved to its own column (variable). However, We have converted a dataset with approx 5321 rows and 27 cols, to one that has 22 rows and 5322 cols. This is slightly smaller and more useful, becasue years are in their own column

```{r, eval=F}
epaExpandTimeSeriesByID = function(epaDataSubset){
    # expand column title into descriptor variables
}
epaReformatTimeSeriesData = function(epaData, byPollutant = T){
    # expand column title into descriptor variables
    # for a group of different columns, using the
    # epaExpandTimeSeriesByID() function, then merge
    # the data into a single output
}
```

#### Comments about extra functionality
These functions Expand the variables in the transposed data summary created
during the data preparation phase of the EPA State summary dataset.
These functiosn are NOT intended to process the ENTIRE dataset, but to prepare
subsets of interest for exploratory analysis and other analyses. Basically,
the functions, particularly:
  epa_help_explore_01epaReformatTimeSeriesData()
were slow when applied to the roughly 5200 variables in the transposed data set.
Further, the datasets tended to be sparse due to missing values, presumably
because (among other reasons) some pollutants were not emmitted by certain sources.