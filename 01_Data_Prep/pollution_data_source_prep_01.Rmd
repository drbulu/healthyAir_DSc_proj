---
title: "Pollution Data Preparation: Strategy and Implementation"
output:
  html_document:
    toc: true
    toc_depth: 4
---

## Introduction

### Chosen dataset

The [selected dataset](https://www.epa.gov/sites/production/files/2016-12/state_tier1_90-16.xls) contained an annual summary of of a number of pollutants by source, pollutant time and state. This dataset was obtained from the [Air Pollutant Emissions Trends page](https://www.epa.gov/air-emissions-inventories/air-pollutant-emissions-trends-data), which also hosted a similar dataset which contained aggregated national figures by source and pollutant. The national dataset covered a longer timeframe but was much less useful because we need state level data in order to integrate pollution data with the asthma and traffic datasets.

Therefore, compared to the [Asthma](https://github.com/drbulu/healthyAir_DSc_proj/blob/master/01_Data_Prep/asthma_data_source_prep_02.Rmd) and [Traffic](https://github.com/drbulu/healthyAir_DSc_proj/blob/master/01_Data_Prep/traffic_data_source_prep_01.Rmd) this challenge was arguably simpler. However, it is important to keep in mind that this dataset has undergone what appears to be a substantial amount of preparation. This was determined by looking at the README excel sheet in the selected state-level dataset which described how the dataset was prepared. Some things to note are:

* This data set was fused from other data sources, as described, and interpolation was used to fill in data availability gaps from years where no source data was available.

* Some definitions change over time and some level of recalculation was required to standardise some values. For example, we may want to merge the "wild fires"", "prescribed fires"" and "misc" variables into a new FIRE_AND_MISC variable, creating a new identifier from the combination of the respective codes.

This is probably one of the reasons why this datset is described as ["trend level data"](https://www.epa.gov/air-emissions-inventories/pollutant-emissions-summary-files-earlier-neis).

However, as mentioned before, the granularity (detail level) is comparable to the other datasets that we aim to look at. In addition, a large number of interesting variables are also included. This, therefore, provides a good introduction to the potential value of this dataset while giving us a needed break from the data crunching grind :scream:.

### Further data sources

#### EPA Pre-Generated Data Files

The section of [Pre-Generated Data Files](https://aqsdr1.epa.gov/aqsweb/aqstmp/airdata/download_files.html) contains some potentially interesting additional data. In particular, the meterological and particulate data in the [daily section](https://aqsdr1.epa.gov/aqsweb/aqstmp/airdata/download_files.html#Daily), and the daily Particulates datasets may be worthwhile investigating as [weather](http://www.abc.net.au/news/2016-11-22/two-die-in-thunderstorm-asthma-emergency-in-melbourne/8044558) and [particulates](http://www.tandfonline.com/doi/abs/10.1080/00039896.1993.9938391) may be linked to respiratory health.

There is a description of the meaning behind the [wind-related variables](https://www.epa.gov/aqs/aqs-wind-speed-and-direction-parameter-change) for reference. If monthly granularity was critical for analysis or if meterological information was required, this archive is a good place to visit. However, a lot of aggregation would be required to get a decent summary by state as outlined above. This maybe worth a look if initial analysis and modelling reveals the need for more specific, or additional information.

#### Comprehensive meterological data

Precipitation data was oddly absent from the Pre-Generated Data Files. However, there is some [weather station](https://www.ncdc.noaa.gov/data-access/land-based-station-data) data that might be useful. The datasets appear to be both rich and raw, including many other parameters, and data is available via FTP with [metadata descriptions](ftp://ftp.ncdc.noaa.gov/pub/data/asos-fivemin/td6401-1.txt). The weather station data potentially useful, but its processing would need to be motivated by a rather compelling reason, given the apparent complexity of the task at hand. 

There also exists other information on [storm events](https://www.ncdc.noaa.gov/stormevents/ftp.jsp) and [hourly precipitation](ftp://ftp.ncdc.noaa.gov/pub/data/hourly_precip-3240/readme.txt).

Lastly, another potentially interesting, comprehensive data source is the [Integrated Surface Database](https://www.ncdc.noaa.gov/isd/data-access) (ISD) [ftp data source] (ftp://ftp.ncdc.noaa.gov/pub/data/noaa/) with a [readme](ftp://ftp.ncdc.noaa.gov/pub/data/noaa/readme.txt), [detailed metadata](ftp://ftp.ncdc.noaa.gov/pub/data/noaa/ish-format-document.pdf) that seems to match individual datasets, [country list](ftp://ftp.ncdc.noaa.gov/pub/data/noaa/country-list.txt), [data description](ftp://ftp.ncdc.noaa.gov/pub/data/noaa/ish-abbreviated.txt)
and a file that appears to contain a [list](ftp://ftp.ncdc.noaa.gov/pub/data/noaa/isd-history.txt) of weather stations. There is is even a [lite-version](ftp://ftp.ncdc.noaa.gov/pub/data/noaa/isd-lite/) that appears to be more user friendly, and thus appealing, dataset according to the [formatting](ftp://ftp.ncdc.noaa.gov/pub/data/noaa/isd-lite/isd-lite-format.txt) and [technical](ftp://ftp.ncdc.noaa.gov/pub/data/noaa/isd-lite/isd-lite-technical-document.txt) documentation. Of all the data source alternatives, this seems to be a good one :triumph: :joy:! 

Basically, there are many more data sources that that could provide more information dimensions to the data that we have obtained thus far. However, these would need further aggregation that would seem excessive for our current purposes.

That said, there is a tutorial for how to process the ISD data [here](http://blue.for.msu.edu/lab-notes/NOAA_0.1-1/NOAA-ws-data.pdf), and one at [r-bloggers](https://www.r-bloggers.com/parse-noaa-integrated-surface-data-files/). Encouragingly, a couple of packages exist that appear to be devoted to analysis of this data include [rnoaa](https://cran.rstudio.com/web/packages/rnoaa/), [isdparser](https://cran.rstudio.com/web/packages/isdparser/).

#### Potential additional data candidates

Overall, the following data sources seem to be most useful to keep in mind when looking to expand the data analysis in future:

* Pre-Generated Data Files: [Daily](https://aqsdr1.epa.gov/aqsweb/aqstmp/airdata/download_files.html#Daily) pollution and partial meterological information (rainfall missing)

* Integrated Surface Database (particularly **ISD-lite**): comprehensive basic [meterological](ftp://ftp.ncdc.noaa.gov/pub/data/noaa/isd-lite/) information.

* Storm events (if not covered in ISD database): metadata [here](ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/ugc_areas.csv).

Upon reflection, the weather data probably deserves its own section (if not project).

## Strategy

Initially, I had thought about leaving the pollution dataset as is. However, I reconsidered on the basis that I wouldn't want to be transfoming the dataset very much (if at all) while trying to explore, model or visualise the dataset. Basically, th helps us to focus on the task at hand.

Inspection of the data gave me the idea to Split source data table into two files: 

* Emissions Data: the main dataset, containing: 
    * the emissions measurements for all the different pollutants of interest.
    * unique columns named by an ecoded combination of location (state), source (tier_1 code) and pollutant

* metadata: contains a condensed set of unique entries that contains
    * item codes: shared with the main dataset
    * item names: complete description of the entities represented by the item codes
    * type descriptor: a description that enables multiple groups of metadata to be described and stored efficiently in a single table.

### Metadata preparation

The groups used to describe the main dataset are Location (state), pollutant and pollution source. Therefore, if we want to crreate metadata tables, we find ourselves with three separate tables consisting of pairs of columns. These tables represent information that describes the dataset that we want to preserve for future use in analyses where we can recombine it with the main dataset in order to describe (annotate) it. The structure of the metadata tables is described as follows:

1. Source location metadata 

<!-- Used traditional HTML tables with custom CSS formatting. -->
<style>.meta-example-a{width:20%; text-align:center;} .meta-example-b{width:50%}</style>

<table class="meta-example-a">
<tr><th>State</th><th>FIPS_CODE</th><tr>
<tr><td>AL</td><td>01</td><tr>
</table>
<br/>

2. Pollutant type metadata

<table class="meta-example-a">
<tr><th>Pollutant</th><th>Unit</th><tr>
<tr><td>NOX</td><td>1000 tons</td><tr>
</table>
<br/>

3. Pollution sources metadata

<table class="meta-example-a">
<tr><th>Source</th><th>Tier1_CODE</th><tr>
<tr><td>AL</td><td>01</td><tr>
</table>
<br/>

From the above examples, we can construct a "master" metadata table to compress the essential descriptive information about the dataset into a single, neat table such as the one below:

<table class="meta-example-b">
<tr><th>Type</th><th>Entity</th><th>Desc</th><tr>
<tr><td>State</td><td>AL</td><td>FIPS_CODE=01</td><tr>
<tr><td>Pollutant</td><td>NOX</td><td>unit=1000 tons</td><tr>
<tr><td>Source</td><td>02</td><td>FUEL COMB. ELEC. UTIL.</td><tr>
</table>
<br/>

<style>.meta-example{width:50%}</style>

This summary metadata effectively condenses the accessory information to a more understand and simplified form, and has some important benefits:

1. Reduces the amount of redundancy in the main data table, thus making the overall file size smaller, even though we have 2 files instead of one.

2. Allows the remaining information to be transposed into a tidy data format ammenable for use in steps such as visulisation using **ggplot2**.

### Main Data Preparation

As a result of the metadata preparation, column names can now be concatenated to be represented as metadata codes, e.g. **STATE.SOURCE.Pollutant**, which would be represented for example as **AL.01.NOX** derived from the sample table above. 

* care needs to be taken to make sure these resulting column names are legal
* can then use the metadata table to link the relevant abbreviations in the main data to additional related information.

Initially, processing requirements for this data set appeared simple. This was due to the fairly processed nature of the dataset. However, the need to prepare each dataset to minimise the need for data manipulation in downstream exploration and analysis complicated things.

The structure of the original data set is not ammenable to this. Therefore, we would need to transpose the data so that the **Year** information is extracted from the table names, and moved to its own column (variable). However, We have converted a dataset with approx 5321 rows and 27 cols, to one that has 22 rows and 5322 cols. This is slightly smaller and more useful, becasue years are in their own column and because metadata has been removed to its own table.

We attempted to regroup the data by pollutant so that the dataset would only need to be 8 columns wide: Year and 7 pollutants. The initial functionality was promisising on a small scale, but was rather inefficient when applied to the 5321 column wide transposed data. However, this useful functionality was salvaged to be used to support exploratory analysis.

## Implementaion

### Main functionality

The code in this section is simply the **pollution_data_source_prep_01.R** data processing script, which relied upon the **helpers-epa_data_prep_01.R** to supply additional functionality.

### Extra functionality

The extra functionality was contained within the **helpers-epa_data_explore_01.R**, which was moved to the exploratory analysis helper function section for future use.

These functions Expand the variables in the transposed data summary created
during the data preparation phase of the EPA State summary dataset.
These functions are NOT intended to process the ENTIRE dataset as described [above](#main-data-preparation), but to prepare subsets of interest for exploratory analysis and other analyses. In addition to being slow, these functions tended to produce sparse datasets, due to missing values, when applied to the whole dataset. Presumably, because (among other reasons) some pollutants were not emmitted by certain sources.

## Updates:

None so far :smile:

<br/>