---
output:
  html_document
---
# Asthma Data Preparation II - Data Processing Implementation

## Introduction

### Background

The [previous document](https://github.com/drbulu/healthyAir_DSc_proj/blob/master/01_Data_Prep/asthma_data_source_prep_01.Rmd) oulined a useful strategy toward the systematic harvesting and processing of the different tables that make up the asthma prevalence dataset that we wish to analyse. To this end, one of the main things that the strategy detailed was the logic by which the individual URLs, that each represent data tables, could be programmatically reconstructed. This is very important because it allows us to reproducibly obtain and analyse this fairly extensive dataset in a **rule-based** manner. This in turn provides the flexibility that is required to: 

* integrate future datasets into this analysis as they become available.
* import and process specific subsets of the data should the whole dataset not be needed.

### Code organisation

#### Scripts

Given the number of functions that would be required to design a data processing framework that makes the strategy easy to follow, and therefore to debug, we will be relying on a combination of two types of scripts:

* **Analysis scripts:** Scripts that perform specific data preparation or analysis tasks on a specific set(s) of data in order to achieve a defined set of goals. Functions that are defined in these scripts are typically and ideally **specific** to the analysis being performed. More generalised functionality can be drawn from onr or more **helper scripts**.
* **Helper scripts:** A set of scripts that contain distinct parts of the functionality needed to perform a set of tasks. These helpers are called upon by **analysis scripts** where needed, in order to make the logic that drives analysis scripts cleaner and easier to follow and manage. The contents of a helper script contains generalised functionality that can be made easily available to muliple **analysis scripts**. 
    * Generalised functionality are basically common functions that perform routine tasks that often need to be performed in multiple scripts. 
    * Helper script files help to improve code (and functionality) reusability and management.

#### Function lists

Another strategy that we are going to use is that of function lists. Basically, storing the helper functions within a list object is quite useful because it:

1) Helps to reduce the Global namespace footprint: lists reduce the number of objects that exist within the main namespace of the working environment, reducing the likelihood and impact of name conflicts. 
    * A small group of objects are easier to give unique names to a larger group.
    * This is of particular note, for example, when a number of external packages need to be loaded
2) Enable ease of functionality management: Basically, this allows easier removal of groups of functions when not required :wink:.

## Phase I: Metadata preparation



<!-- 
    DESCRIBE STRATEGY - NOT DETAILS.. refer to scripts for that!

Basically these sections should flow as follows:
1. Strategy description: dot-point brief
2. Brief reference to files responsible for strategy implementation

In depth description can be confined to the relevant scripts

--->


The first part of the process is metadata preparation. For the purposes of this section, metadata describes one or more tables (data.frames) that contain an organised set of URLs and related information. This is important because we are going to use this prepared metadata to systematically acquire the raw data that we will then process into our [tidy dataset](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html). Basically, the aim of this section is to create a family of functions to facilitate metadata preparation:

1. Construct a metadata root (or core) table: 

Such a table would provide the essential information needed to construct all of the different <u>combinations</u> of URLs that need to be constructed for each year of interest. The following table, which is an extension of the one in [this section](https://github.com/drbulu/healthyAir_DSc_proj/blob/master/01_Data_Prep/asthma_data_source_prep_01.Rmd#demographic-attributes), was constructed from analysis of the different tables and meta information about these tables:

| demographic     | demID | hasChildData |
|-----------------|-------|--------------|
| Overall         | 1     | Y            |
| Gender          | 21    | Y            |
| Age             | 3     | Y            |
| Ethnicity       | 5     | Y            |
| Education       | 6     | N            |
| Income          | 7     | N            |

* the **demographic** and **demID** columns relate to a specific attribute and the associated ID that is used to represent it in the URL table name.
* the **hasChildData** column indicates whether data for a particular demographic is available for the "Child" data group. All data was available for the "Adult" group.

2. Create functions to construct valid individual URLs for a particular year from the core metadata table based on whether the data of the URL pertains to:
        * either the Adult or Child group
        * either the current or archive data series
        * current or lifetime asthma prevalence
    
3. Construct functions to create tables of URLs based on the individual URL creation files. These tables are organised into a single large list for efficiency.

## Phase II: Data processing from metadata

There are potentially many ways of organising this part of the process, but the following workflow is relatively straightforward. We could approach this part of the process in one of two basic ways:

* We could first obtain the raw data and store it as relatively raw tables to disk in a simple format, such as [CSV](https://en.wikipedia.org/wiki/Comma-separated_values). The raw data files could then be imported back into the data analysis framework, R in this case, for further processing. 
* We could extract and process the tablular data from the URLs sequentially without file download.

We will be pursuing the first approach as the total data acquired was substantial, but not unmanageable in memory. However, the first approach is still a good idea, and one that we could explore later. 

In order to achieve our objective, we will develop the code according to the following outline:

1. Obtain a single data table from HTML source code.

2. Obtain a list of data tables from a single URL metadata table.

3. Proces each list of data tables to create a single _data series_ data series: a combination of a group and demographic subgroup. e.g. Adult (group) / Income (demographic subgroup). <b style="color:blue;">optional:</b> save the raw tables to disk.

4. Combine as set of data series into a single master list. <b style="color:blue;">optional:</b> get the raw tables form disk.

5. Save the tables to disk.

### implementation Summary

The metadata preparation and data acquisition code relied heavily on "batch" functions that applied a small number of more specific functions over a range of groups or years. The code is organised so that the data processing scripts handle the batch processing of data, call a few "master" functions from the helper scripts, which themselves recruit other helpers upon which they have a [dependency](https://en.wikipedia.org/wiki/Coupling_(computer_programming)).

The code that supports the preparation of the Asthma dataset are as follows:

1. Data processing code: **asthma_data_source_prep_01.R**

2. Phase 1: Functionality stored in the helper file: **helpers-asthma_data_01-url_table_prep.R**.

3. Phase 2: Functionality stored in the helper file: **helpers-asthma_data_02-url_data_prep.R**.

4. Optional code: Helpful for developlment

* Prototyping test script: **asthma_data_source_prep_test_01.R**. Very rough, slightly repetitive "scrap book" to help develop the Phase 2 data processing code.

* **helpers-asthma_data_02b-url_data_test.R**: contains one useful function for obtaining subsets of URL tables for debugging purposes.

## Next Steps

Now that we have a tidy Asthma dataset, we will proceed to tackle the next dataset... [traffic](https://github.com/drbulu/healthyAir_DSc_proj/blob/master/01_Data_Prep/traffic_data_source_prep_01.Rmd) :wink:!

## Updates:

None so far :smile:

<br/>