---
output:
  html_document:
    keep_md: yes
---
# Asthma Data Preparation II - Data Processing Implementation

## Introduction

The strategy document oulined a useful strategy toward the systematic harvesting and processing of the different tables that make up the asthma prevalence dataset that we wish to analyse. To this end, one of the main things that the strategy detailed was the logic by which the individual URLs, that each represent data tables, could be programmatically reconstructed. This is very important because it allows us to reproducibly obtain and analyse this fairly extensive dataset in a **rule-based** manner. This in turn provides the flexibility that is required to: 

* integrate future datasets into this analysis as they become available.
* import and process specific subsets of the data should the whole dataset not be needed.

Given the number of functions that would be required to design a data processing framework that makes the strategy easy to follow, and therefore to debug, we will be relying on a combination of two types of scripts:

* **Analysis scripts:** Scripts that perform specific data preparation or analysis tasks on a specific set(s) of data in order to achieve a defined set of goals. Functions that are defined in these scripts are typically and ideally **specific** to the analysis being performed. More generalised functionality can be drawn from onr or more **helper scripts**.
* **Helper scripts:** A set of scripts that contain distinct parts of the functionality needed to perform a set of tasks. These helpers are called upon by **analysis scripts** where needed, in order to make the logic that drives analysis scripts cleaner and easier to follow and manage. The contents of a helper script contains generalised functionality that can be made easily available to muliple **analysis scripts**. 
    * Generalised functionality are basically common functions that perform routine tasks that often need to be performed in multiple scripts. 
    * Helper script files help to improve code (and functionality) reusability and management.

Another strategy that we are going to use is that of function lists. Basically, storing the helper functions within a list object is quite useful because it:

1) Helps to reduce the Global namespace footprint: lists reduce the number of objects that exist within the main namespace of the working environment, reducing the likelihood and impact of name conflicts. 
    * A small group of objects are easier to give unique names to a larger group.
    * This is of particular note, for example, when a number of external packages need to be loaded
2) Enable ease of functionality management: Basically, this allows easier removal of groups of functions when not required :wink:.

## Phase I: Metadata preparation

The first part of the process is metadata preparation. For the purposes of this section, metadata describes one or more tables (data.frames) that contain an organised set of URLs and related information.

The aim of this section is to create a family of functions to facilitate metadata preparation. 

1) helper function list

asthma_helper_func_01 = list()

2) Metadata table generation 

asthma_helper_func_01$initAsthmaMetaData

3) Systematic construction URLs from metadata components

    a. Core function for producing URLs from Current data archive

asthma_helper_func_01$getCurrentAsthmaURL

    b. Core function for producing URLs from Archive data archive

asthma_helper_func_01$getArchiveAsthmaURL

4) Create a single URL metadata table

Make a data frame to make data organisation more simple and useful :)
This is MUCH better than getting a large number of lists and/or vectors
also means that I can inject fewer variables into the equation :)
This function gets the tables for ONE specific demographic (demID) for
all of the years specified in yearSeries for the chosen subset (isAdultData)

asthma_helper_func_01$prepAsthmaYearURLsByID

5) Create a list of URL data tables based on input information: 

Designed to obtain
both adult and child data from the default metadata table for ease of use by 
abstraction of underlying details. User needs to call this function without
arguments to get all the data. The "endYear" variable can be used to change the 
recency of the data collected to restrict data series size or to include future data
as required. Further refinements could be made, but it should suffice to keep this 
function relatively simple to avoid unecessary complexity and resulting logical error.

asthma_helper_func_01$createAsthmaURLTables()

Functionality stored in the helper file: **helpers-asthma_data_01-url_table_prep.R**.

## Phase II: Data processing from metadata

There are many ways of organising this, but the following workflow

Functionality required:

1) Obtain a single data table from HTML source code -->
2) Obtain a list of data tables from a single URL metadata table
3) Proces each list of data tables to create a single _data series_
    data series: a combination of a group and demographic subgroup.
    e.g. Adult (group) / Income (demographic subgroup)
    optional: save the list of tables to disc.
4) Combine as set of data series into a single master list.
    optional: get the tables form disc
5) Save the tables to disc

Functionality stored in the helper file: **helpers-asthma_data_02-url_data_prep.R**.

<b style="color:red;">Note:</b> Needed to run test script to develop and refine table cleaning functinality. **asthma_data_source_prep_test_01.R**.

### Note:
Data can be saved to disc as simple plain text CSV or tab delimeted tables, or as Excel spreadsheets. pros and cons.

## Next Steps

## Implemenation

asthma_data_source_prep_01.R (method 1: sequential data processing)

asthma_data_source_prep_02.R (method 2: now with file IO)

Use of web scraping tools such as [rvest](https://blog.rstudio.org/2014/11/24/rvest-easy-web-scraping-with-r/) in combination with the knowledge gained above to generate script(s) to acquire and process the data for further analysis. :smile:

future note: Need to check table footnotes for "surprises" :wink:!

<br/>