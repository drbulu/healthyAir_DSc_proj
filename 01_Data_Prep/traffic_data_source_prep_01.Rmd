---
title: "Traffic Data Preparation: Strategy and Implementation"
output:
  html_document:
    toc: true
    toc_depth: 4
--- 

## Introduction

I learned valuable lessons from the preparation of the Asthma dataset regarding both [strategy](https://github.com/drbulu/healthyAir_DSc_proj/blob/master/01_Data_Prep/asthma_data_source_prep_01.Rmd) and [implementation](https://github.com/drbulu/healthyAir_DSc_proj/blob/master/01_Data_Prep/asthma_data_source_prep_02.Rmd) considerations. However, there were some important considerations when working out how to tackle the data processing challenge for this raw dataset. 

The main page [here](https://www.fhwa.dot.gov/policyinformation/travel_monitoring/tvt.cfm) contains a list of tables that each contain links to monthly traffic data over a number of years. The two basic groups of data listed on this page are: 

1. **Older Data** (1970 to 2002): These are rather condensed summary data with a level of detail not sufficient for our purposes. Therefore, we will ignore this series.

2. **Current Data** (2002 to 2014): Monthly data series of interest.

The set of URLs below represent some of the different forms that the data URLs of interest:

<pre>
* https://www.fhwa.dot.gov/<b style="color:red;">policyinformation/travel_monitoring/</b>16novtvt/16novtvt.xls
* https://www.fhwa.dot.gov/ohim/tvtw/07dectvt/07dectvt.<b style="color:red;">xlsx</b>
* https://www.fhwa.dot.gov/ohim/tvtw/06jantvt/jan<b style="color:red;">06</b>tvt.xls
* https://www.fhwa.dot.gov/ohim/tvtw/03dectvt/03dectvt.xls
* https://www.fhwa.dot.gov/ohim/tvtw/03jultvt/<b style="color:red;">03</b>jultvt.xls
* https://www.fhwa.dot.gov/ohim/tvtw/03juntvt/tvtjun<b style="color:red;">03</b>.xls
* https://www.fhwa.dot.gov/ohim/tvtw/02jantvt/tvtjan<b style="color:red;">02</b>.xls
</pre>

The hope was to find, as we did in the Asthma data URLs, a useful pattern that we could exploit to programmatically reconstruct these URLs prior to file download and processing. The **&ast;.xls** (and &ast;.xlsx) files were the natural choice to download, given the availability of tools in R, such as xlsx and readxl, to perform data extraction from these files.

However, as you can see above, there were numerous quirks, some of which could have proved difficult (certainly tedious) to resolve. This is in addition to the risks of manually trying to account for these apparently random deviations. For example:
 * for no apparent reason, we have one (and only one as it turned out) xlsx file in the URL set.. what the ! :astonished:
* more frustratingly, construction of the file names changes, seemingly without reason :scream: !

Yeah... done with that :alien: ! I felt that this was enough of a mess that I had to rethink my strategy entirely... 

The solution? Directly extract the URLs from the table data elements and compile them into a metadata URL for subsequent download. This approach had the distinct beauty of avoiding the vaguaries of squeezing useful patterns out of the [non sequitur](https://en.wikipedia.org/wiki/Non_sequitur_(logic)) that is the traffic data URL structure logic.

## Strategy

There are potentially many ways of organising this part of the process, but the following workflow is relatively straightforward. Importantly, this phase separates the downloading (aquisition) of the data from the processing of the data into a tidy data format. The reason for this lies in the large number of files to be downloaded.

1. Harvest links: The first task was to try to obtain all of the links directly from the [main page](https://www.fhwa.dot.gov/policyinformation/travel_monitoring/tvt.cfm) so that they could be cleaned and organised into one or more metadata table of URLs. The solution that I went with to obtain all the links information was [this one](http://stackoverflow.com/questions/31924546/rvest-table-scraping-including-links). 

2. Download excel files: For this part of the process, the links were prepared to validate them as some URLs were malformed during the web scraping process. Then the cleaned URLs were downloaded to a specified directory for further processing.

* One consideration was the control of loop execution timing by [introducing a delay](http://stackoverflow.com/questions/1174799/how-to-make-execution-pause-sleep-wait-for-x-seconds-in-r). The reason was to have a means of controlling file download frequency, knowing that some data providers aren't keen on being hammered by multiple requests from a single source. In this case there was no problem, but it is a good strategy to bear in mind. 

* Another consideration was to check for the existence of a file in the event that the script needs to be rerun. In future it would be potentially useful to allow files to be overwritten. However, deleting the offending files was sufficient.

3. Create tidy dataset: Obtain the downloaded files and process them into a single master file to be saved in an output folder.

## Implementation

**traffic_data_source_prep_01.R**: Data processin script for metadata preparation and file manipulation. This script was responsible for processing input files using the available metadata to create single tidy data table that was downloaded to disc.

**general_helpers_01.R**: This useful script contained helpful functions to perform web scraping, file download and file I/O. These functions were deemed so generally useful that they would be useful across the entire project. Therefore, the creation of a general helper enabled this functionality to be easily called and added to.

**helpers-traffic_data_prep_01.R**: helper script for provide functinality for metadata preparation and file download.

**helpers-traffic_data_prep_02.R**: helper script to handle excel file import and processing into tidy data. I did some basic benchmarking using proc.time() (as discussed [here](http://stackoverflow.com/questions/6262203/#17802487) for example). This revealed that there was much better performance using [readxl](https://cran.r-project.org/web/packages/readxl/index.html) functions over [xlsx](https://cran.r-project.org/web/packages/xlsx/index.html) for reading the downloaded files into R. Interestingly, processing of the lone xlsx file was faster once this update was performed! Previously, there was a noticable stall when that file was processed. Printing progress to the console using cat() was very useful for progress analysis and debugging. For more one benchmarking there are packages such as [microbenchmark](https://cran.r-project.org/web/packages/microbenchmark/index.html) and [rbenchmark](https://cran.r-project.org/web/packages/rbenchmark/) would be interesting to explore in future.

**traffic_data_source_prep_01_test.R**: This version of the traffic data preparation script was used as a testing "scrap book" to develop code in the helper functions. It is a bit messy and repetitive, but instrumental in working through the various teething problems and optiomisations required to create the final script.

## Next Steps

Now that we have a tidy Traffic dataset, the next task was to tackle the processing of the [pollution](https://github.com/drbulu/healthyAir_DSc_proj/blob/master/01_Data_Prep/pollution_data_source_prep_01.Rmd) dataset :wink:!

## Updates:

None so far :smile:

<br/>